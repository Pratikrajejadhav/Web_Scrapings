{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Web driver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "def scrape_amazon_reviews(url, max_page=5):\n",
    "    reviews = []\n",
    "    for page in range(1, max_page + 1):\n",
    "        # print(\"Page:\", page)\n",
    "        page_url = f'{url}{page}?ie=UTF8&reviewerType=all_reviews&pageNumber={page}'\n",
    "        driver.get(page_url)\n",
    "\n",
    "        review_elements = driver.find_elements(By.XPATH, \"//div[@data-hook='review']\")\n",
    "        if review_elements:\n",
    "            for review in review_elements:\n",
    "                review_data_dict = {}\n",
    "                review_data_dict['reviewer_name'] = review.find_element(By.XPATH, \".//span[@class='a-profile-name']\").text\n",
    "                review_data_dict['review_date'] = review.find_element(By.XPATH, \".//span[@data-hook='review-date']\").text\n",
    "                review_data_dict['rating'] = review.find_element(By.XPATH, \".//i[contains(@class, 'review-rating')]/span\").get_attribute('innerText')\n",
    "                review_data_dict['review_text'] = review.find_element(By.XPATH, \".//span[@data-hook='review-body']\").text\n",
    "                reviews.append(review_data_dict)\n",
    "        else:\n",
    "            print(f'NO Reviews Found on Page {page}')\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            next_button = driver.find_element(By.XPATH, \"//li[@class='a-last']//a\")\n",
    "            next_button.click()\n",
    "            WebDriverWait(driver, 10).until(EC.url_changes(page_url))\n",
    "        except NoSuchElementException:\n",
    "            print(\"Next Page not found\")\n",
    "            break\n",
    "\n",
    "    return reviews\n",
    "\n",
    "def scrape_flipkart_reviews(url, max_page=5):\n",
    "    reviews = []\n",
    "    headers = {\n",
    "        'User-Agent': 'Use your own user agent',\n",
    "        'Accept-Language': 'en-us,en;q=0.5'\n",
    "    }\n",
    "\n",
    "    for i in range(1, max_page + 1):\n",
    "        page_url = f'{url}{i}'\n",
    "        page = requests.get(page_url, headers=headers)\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "        names = soup.find_all('p', class_='_2NsDsF AwS1CA')\n",
    "        titles = soup.find_all('p', class_='z9E0IG')\n",
    "        ratings = soup.find_all('div', class_=['XQDdHH Ga3i8K', 'XQDdHH Czs3gR Ga3i8K' , 'XQDdHH Js30Fc Ga3i8K'])\n",
    "        comments = soup.find_all('div', class_='ZmyHeo')\n",
    "\n",
    "        for name, title, rating, comment in zip(names, titles, ratings, comments):\n",
    "            review_data_dict = {}\n",
    "            review_data_dict['reviewer_name'] = name.get_text()\n",
    "            review_data_dict['review_title'] = title.get_text()\n",
    "            review_data_dict['rating'] = rating.get_text() if rating else '0'\n",
    "            review_data_dict['review_text'] = comment.div.div.get_text(strip=True)\n",
    "            reviews.append(review_data_dict)\n",
    "\n",
    "    return reviews\n",
    "\n",
    "amazon_url = \"https://www.amazon.in/Apple-iPhone-13-128GB-Blue/product-reviews/B09G9BL5CP/ref=cm_cr_arp_d_paging_btm_next_\"\n",
    "flipkart_url = \"https://www.flipkart.com/apple-iphone-15-plus-black-256-gb/product-reviews/itm4b0608e773fc5?pid=MOBGTAGPWKT2VSBB&lid=LSTMOBGTAGPWKT2VSBBYV0FGC&marketplace=FLIPKART&page=\"\n",
    "\n",
    "amazon_reviews = scrape_amazon_reviews(amazon_url)\n",
    "flipkart_reviews = scrape_flipkart_reviews(flipkart_url)\n",
    "\n",
    "driver.close()\n",
    "\n",
    "df_amazon = pd.DataFrame(amazon_reviews)\n",
    "df_flipkart = pd.DataFrame(flipkart_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewer_name</th>\n",
       "      <th>review_date</th>\n",
       "      <th>rating</th>\n",
       "      <th>review_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jnan Shetty</td>\n",
       "      <td>Reviewed in India on 30 December 2023</td>\n",
       "      <td>5.0 out of 5 stars</td>\n",
       "      <td>Play Video\\nThis is my first iPhone and it alw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Amazing performance. A15 Bio chip prosessor is...</td>\n",
       "      <td>Reviewed in India on 16 April 2024</td>\n",
       "      <td>5.0 out of 5 stars</td>\n",
       "      <td>The camera quality is just üî•.\\nThe battery is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Keshav Maheshwari</td>\n",
       "      <td>Reviewed in India on 4 February 2023</td>\n",
       "      <td>4.0 out of 5 stars</td>\n",
       "      <td>This is my first iPhone and it always feels go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pawan Sasane</td>\n",
       "      <td>Reviewed in India on 8 May 2024</td>\n",
       "      <td>5.0 out of 5 stars</td>\n",
       "      <td>Great Purchase! iPhone 13 is awesome phone .. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Happy</td>\n",
       "      <td>Reviewed in India on 22 March 2024</td>\n",
       "      <td>5.0 out of 5 stars</td>\n",
       "      <td>The big difference between ios and android is ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       reviewer_name  \\\n",
       "0                                        Jnan Shetty   \n",
       "1  Amazing performance. A15 Bio chip prosessor is...   \n",
       "2                                  Keshav Maheshwari   \n",
       "3                                       Pawan Sasane   \n",
       "4                                              Happy   \n",
       "\n",
       "                             review_date              rating  \\\n",
       "0  Reviewed in India on 30 December 2023  5.0 out of 5 stars   \n",
       "1     Reviewed in India on 16 April 2024  5.0 out of 5 stars   \n",
       "2   Reviewed in India on 4 February 2023  4.0 out of 5 stars   \n",
       "3        Reviewed in India on 8 May 2024  5.0 out of 5 stars   \n",
       "4     Reviewed in India on 22 March 2024  5.0 out of 5 stars   \n",
       "\n",
       "                                         review_text  \n",
       "0  Play Video\\nThis is my first iPhone and it alw...  \n",
       "1  The camera quality is just üî•.\\nThe battery is ...  \n",
       "2  This is my first iPhone and it always feels go...  \n",
       "3  Great Purchase! iPhone 13 is awesome phone .. ...  \n",
       "4  The big difference between ios and android is ...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_amazon.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewer_name</th>\n",
       "      <th>review_title</th>\n",
       "      <th>rating</th>\n",
       "      <th>review_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sagar Behera</td>\n",
       "      <td>Best in the market!</td>\n",
       "      <td>5</td>\n",
       "      <td>Go fr it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Anshul Duhan</td>\n",
       "      <td>Must buy!</td>\n",
       "      <td>5</td>\n",
       "      <td>Best in class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gundabattina SaradhiMuneendra</td>\n",
       "      <td>Mind-blowing purchase</td>\n",
       "      <td>5</td>\n",
       "      <td>Fabulous üòçLoved itCamera awesome üòòPerformance ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ashutosh  Singh</td>\n",
       "      <td>Must buy!</td>\n",
       "      <td>5</td>\n",
       "      <td>Blue colour is very lightBut performance is ve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Anirudhya  Ghosh</td>\n",
       "      <td>Worth every penny</td>\n",
       "      <td>5</td>\n",
       "      <td>Premium Colour</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   reviewer_name           review_title rating  \\\n",
       "0                   Sagar Behera    Best in the market!      5   \n",
       "1                   Anshul Duhan              Must buy!      5   \n",
       "2  Gundabattina SaradhiMuneendra  Mind-blowing purchase      5   \n",
       "3                Ashutosh  Singh              Must buy!      5   \n",
       "4               Anirudhya  Ghosh      Worth every penny      5   \n",
       "\n",
       "                                         review_text  \n",
       "0                                           Go fr it  \n",
       "1                                      Best in class  \n",
       "2  Fabulous üòçLoved itCamera awesome üòòPerformance ...  \n",
       "3  Blue colour is very lightBut performance is ve...  \n",
       "4                                     Premium Colour  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_flipkart.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing test.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test.py\n",
    "import streamlit as st\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "# Web driver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "def scrape_amazon_reviews(url, max_page=5):\n",
    "    reviews = []\n",
    "    for page in range(1, max_page + 1):\n",
    "        print(\"Page:\", page)\n",
    "        page_url = f'{url}{page}?ie=UTF8&reviewerType=all_reviews&pageNumber={page}'\n",
    "        driver.get(page_url)\n",
    "\n",
    "        review_elements = driver.find_elements(By.XPATH, \"//div[@data-hook='review']\")\n",
    "        if review_elements:\n",
    "            for review in review_elements:\n",
    "                review_data_dict = {}\n",
    "                review_data_dict['reviewer_name'] = review.find_element(By.XPATH, \".//span[@class='a-profile-name']\").text\n",
    "                review_data_dict['review_date'] = review.find_element(By.XPATH, \".//span[@data-hook='review-date']\").text\n",
    "                review_data_dict['rating'] = review.find_element(By.XPATH, \".//i[contains(@class, 'review-rating')]/span\").get_attribute('innerText')\n",
    "                review_data_dict['review_text'] = review.find_element(By.XPATH, \".//span[@data-hook='review-body']\").text\n",
    "                reviews.append(review_data_dict)\n",
    "        else:\n",
    "            print(f'NO Reviews Found on Page {page}')\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            next_button = driver.find_element(By.XPATH, \"//li[@class='a-last']//a\")\n",
    "            next_button.click()\n",
    "            WebDriverWait(driver, 10).until(EC.url_changes(page_url))\n",
    "        except NoSuchElementException:\n",
    "            print(\"Next Page not found\")\n",
    "            break\n",
    "\n",
    "    return reviews\n",
    "\n",
    "def scrape_flipkart_reviews(url, max_page=5):\n",
    "    reviews = []\n",
    "    headers = {\n",
    "        'User-Agent': 'Use your own user agent',\n",
    "        'Accept-Language': 'en-us,en;q=0.5'\n",
    "    }\n",
    "\n",
    "    for i in range(1, max_page + 1):\n",
    "        page_url = f'{url}{i}'\n",
    "        page = requests.get(page_url, headers=headers)\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "        names = soup.find_all('p', class_='_2NsDsF AwS1CA')\n",
    "        titles = soup.find_all('p', class_='z9E0IG')\n",
    "        ratings = soup.find_all('div', class_=['XQDdHH Ga3i8K', 'XQDdHH Czs3gR Ga3i8K' , 'XQDdHH Js30Fc Ga3i8K'])\n",
    "        comments = soup.find_all('div', class_='ZmyHeo')\n",
    "\n",
    "        for name, title, rating, comment in zip(names, titles, ratings, comments):\n",
    "            review_data_dict = {}\n",
    "            review_data_dict['reviewer_name'] = name.get_text()\n",
    "            review_data_dict['review_title'] = title.get_text()\n",
    "            review_data_dict['rating'] = rating.get_text() if rating else '0'\n",
    "            review_data_dict['review_text'] = comment.div.div.get_text(strip=True)\n",
    "            reviews.append(review_data_dict)\n",
    "\n",
    "    return reviews\n",
    "\n",
    "# Streamlit code\n",
    "st.title('Web Scraping App')\n",
    "\n",
    "option = st.sidebar.selectbox(\n",
    "    'Which website do you want to scrape?',\n",
    "    ('Amazon', 'Flipkart')\n",
    ")\n",
    "\n",
    "url = st.sidebar.text_input('Enter the URL of the product')\n",
    "\n",
    "if st.sidebar.button('Scrape'):\n",
    "    if option == 'Amazon':\n",
    "        reviews = scrape_amazon_reviews(url)\n",
    "    else:\n",
    "        reviews = scrape_flipkart_reviews(url)\n",
    "\n",
    "    df = pd.DataFrame(reviews)\n",
    "    st.write(df)\n",
    "\n",
    "\n",
    "\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# with sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_senti.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_senti.py\n",
    "import streamlit as st\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import google.generativeai as genai\n",
    "import google.ai.generativelanguage as glm\n",
    "import pandas as pd\n",
    "\n",
    "genai.configure(api_key=\"<your api key>\")\n",
    "\n",
    "\n",
    "# Web driver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "def scrape_amazon_reviews(url, max_page=5):\n",
    "    reviews = []\n",
    "    for page in range(1, max_page + 1):\n",
    "        print(\"Page:\", page)\n",
    "        page_url = f'{url}{page}?ie=UTF8&reviewerType=all_reviews&pageNumber={page}'\n",
    "        driver.get(page_url)\n",
    "\n",
    "        review_elements = driver.find_elements(By.XPATH, \"//div[@data-hook='review']\")\n",
    "        if review_elements:\n",
    "            for review in review_elements:\n",
    "                review_data_dict = {}\n",
    "                review_data_dict['reviewer_name'] = review.find_element(By.XPATH, \".//span[@class='a-profile-name']\").text\n",
    "                review_data_dict['review_date'] = review.find_element(By.XPATH, \".//span[@data-hook='review-date']\").text\n",
    "                review_data_dict['rating'] = review.find_element(By.XPATH, \".//i[contains(@class, 'review-rating')]/span\").get_attribute('innerText')\n",
    "                review_data_dict['review_text'] = review.find_element(By.XPATH, \".//span[@data-hook='review-body']\").text\n",
    "                reviews.append(review_data_dict)\n",
    "        else:\n",
    "            print(f'NO Reviews Found on Page {page}')\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            next_button = driver.find_element(By.XPATH, \"//li[@class='a-last']//a\")\n",
    "            next_button.click()\n",
    "            WebDriverWait(driver, 10).until(EC.url_changes(page_url))\n",
    "        except NoSuchElementException:\n",
    "            print(\"Next Page not found\")\n",
    "            break\n",
    "\n",
    "    return reviews\n",
    "\n",
    "def scrape_flipkart_reviews(url, max_page=5):\n",
    "    reviews = []\n",
    "    headers = {\n",
    "        'User-Agent': 'Use your own user agent',\n",
    "        'Accept-Language': 'en-us,en;q=0.5'\n",
    "    }\n",
    "\n",
    "    for i in range(1, max_page + 1):\n",
    "        page_url = f'{url}{i}'\n",
    "        page = requests.get(page_url, headers=headers)\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "        names = soup.find_all('p', class_='_2NsDsF AwS1CA')\n",
    "        titles = soup.find_all('p', class_='z9E0IG')\n",
    "        ratings = soup.find_all('div', class_=['XQDdHH Ga3i8K', 'XQDdHH Czs3gR Ga3i8K' , 'XQDdHH Js30Fc Ga3i8K'])\n",
    "        comments = soup.find_all('div', class_='ZmyHeo')\n",
    "\n",
    "        for name, title, rating, comment in zip(names, titles, ratings, comments):\n",
    "            review_data_dict = {}\n",
    "            review_data_dict['reviewer_name'] = name.get_text()\n",
    "            review_data_dict['review_title'] = title.get_text()\n",
    "            review_data_dict['rating'] = rating.get_text() if rating else '0'\n",
    "            review_data_dict['review_text'] = comment.div.div.get_text(strip=True)\n",
    "            reviews.append(review_data_dict)\n",
    "\n",
    "    return reviews\n",
    "\n",
    "# Streamlit code\n",
    "st.title('Web Scraping App')\n",
    "\n",
    "option = st.sidebar.selectbox(\n",
    "    'Which website do you want to scrape?',\n",
    "    ('Amazon', 'Flipkart')\n",
    ")\n",
    "\n",
    "url = st.sidebar.text_input('Enter the URL of the product')\n",
    "\n",
    "if st.sidebar.button('Scrape'):\n",
    "    if option == 'Amazon':\n",
    "        reviews = scrape_amazon_reviews(url)\n",
    "    else:\n",
    "        reviews = scrape_flipkart_reviews(url)\n",
    "\n",
    "    df = pd.DataFrame(reviews)\n",
    "\n",
    "    # st.write(df)\n",
    "\n",
    "    # st.header(\"Sentiment Analysis of Reviews\")\n",
    "    data = df\n",
    "    sentiment = []\n",
    "    for reviews in df['review_text']:\n",
    "\n",
    "             # model selection\n",
    "        model = genai.GenerativeModel('gemini-pro')\n",
    "\n",
    "        prompt = \"\"\" Give the Sentiment analysis of given review only in two words either ***POSITIVE** üòÄüòÄ  or **NEGATIVE** ‚òπÔ∏è‚òπÔ∏è \"\"\"\n",
    "\n",
    "        response = model.generate_content([prompt, reviews])\n",
    "            #  st.write(reviews)\n",
    "            #  st.write(response.text)\n",
    "        sentiment.append(response.text)\n",
    "            #  st.write(\"=====================================================================================\")\n",
    "    df['Sentiment'] = sentiment \n",
    "\n",
    "    st.write(df)\n",
    "\n",
    "\n",
    "\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
