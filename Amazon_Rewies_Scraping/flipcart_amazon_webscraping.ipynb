{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Web driver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "def scrape_amazon_reviews(url, max_page=5):\n",
    "    reviews = []\n",
    "    for page in range(1, max_page + 1):\n",
    "        # print(\"Page:\", page)\n",
    "        page_url = f'{url}{page}?ie=UTF8&reviewerType=all_reviews&pageNumber={page}'\n",
    "        driver.get(page_url)\n",
    "\n",
    "        review_elements = driver.find_elements(By.XPATH, \"//div[@data-hook='review']\")\n",
    "        if review_elements:\n",
    "            for review in review_elements:\n",
    "                review_data_dict = {}\n",
    "                review_data_dict['reviewer_name'] = review.find_element(By.XPATH, \".//span[@class='a-profile-name']\").text\n",
    "                review_data_dict['review_date'] = review.find_element(By.XPATH, \".//span[@data-hook='review-date']\").text\n",
    "                review_data_dict['rating'] = review.find_element(By.XPATH, \".//i[contains(@class, 'review-rating')]/span\").get_attribute('innerText')\n",
    "                review_data_dict['review_text'] = review.find_element(By.XPATH, \".//span[@data-hook='review-body']\").text\n",
    "                reviews.append(review_data_dict)\n",
    "        else:\n",
    "            print(f'NO Reviews Found on Page {page}')\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            next_button = driver.find_element(By.XPATH, \"//li[@class='a-last']//a\")\n",
    "            next_button.click()\n",
    "            WebDriverWait(driver, 10).until(EC.url_changes(page_url))\n",
    "        except NoSuchElementException:\n",
    "            print(\"Next Page not found\")\n",
    "            break\n",
    "\n",
    "    return reviews\n",
    "\n",
    "def scrape_flipkart_reviews(url, max_page=5):\n",
    "    reviews = []\n",
    "    headers = {\n",
    "        'User-Agent': 'Use your own user agent',\n",
    "        'Accept-Language': 'en-us,en;q=0.5'\n",
    "    }\n",
    "\n",
    "    for i in range(1, max_page + 1):\n",
    "        page_url = f'{url}{i}'\n",
    "        page = requests.get(page_url, headers=headers)\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "        names = soup.find_all('p', class_='_2NsDsF AwS1CA')\n",
    "        titles = soup.find_all('p', class_='z9E0IG')\n",
    "        ratings = soup.find_all('div', class_=['XQDdHH Ga3i8K', 'XQDdHH Czs3gR Ga3i8K' , 'XQDdHH Js30Fc Ga3i8K'])\n",
    "        comments = soup.find_all('div', class_='ZmyHeo')\n",
    "\n",
    "        for name, title, rating, comment in zip(names, titles, ratings, comments):\n",
    "            review_data_dict = {}\n",
    "            review_data_dict['reviewer_name'] = name.get_text()\n",
    "            review_data_dict['review_title'] = title.get_text()\n",
    "            review_data_dict['rating'] = rating.get_text() if rating else '0'\n",
    "            review_data_dict['review_text'] = comment.div.div.get_text(strip=True)\n",
    "            reviews.append(review_data_dict)\n",
    "\n",
    "    return reviews\n",
    "\n",
    "amazon_url = \"https://www.amazon.in/Apple-iPhone-13-128GB-Blue/product-reviews/B09G9BL5CP/ref=cm_cr_arp_d_paging_btm_next_\"\n",
    "flipkart_url = \"https://www.flipkart.com/apple-iphone-15-plus-black-256-gb/product-reviews/itm4b0608e773fc5?pid=MOBGTAGPWKT2VSBB&lid=LSTMOBGTAGPWKT2VSBBYV0FGC&marketplace=FLIPKART&page=\"\n",
    "\n",
    "amazon_reviews = scrape_amazon_reviews(amazon_url)\n",
    "flipkart_reviews = scrape_flipkart_reviews(flipkart_url)\n",
    "\n",
    "driver.close()\n",
    "\n",
    "df_amazon = pd.DataFrame(amazon_reviews)\n",
    "df_flipkart = pd.DataFrame(flipkart_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewer_name</th>\n",
       "      <th>review_date</th>\n",
       "      <th>rating</th>\n",
       "      <th>review_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pankaj Kumar</td>\n",
       "      <td>Reviewed in India on 24 February 2024</td>\n",
       "      <td>5.0 out of 5 stars</td>\n",
       "      <td>The iPhone 13 128GB has surpassed my expectati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vaibhav</td>\n",
       "      <td>Reviewed in India on 13 November 2023</td>\n",
       "      <td>5.0 out of 5 stars</td>\n",
       "      <td>I snagged the iPhone 13 during the Great India...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ApTreX</td>\n",
       "      <td>Reviewed in India on 6 February 2022</td>\n",
       "      <td>5.0 out of 5 stars</td>\n",
       "      <td>My honest review after going broke buying this...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Faiyaz</td>\n",
       "      <td>Reviewed in India on 5 January 2024</td>\n",
       "      <td>4.0 out of 5 stars</td>\n",
       "      <td>Design:\\nThe iPhone 13 retains the iconic desi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Avnish Shukla</td>\n",
       "      <td>Reviewed in India on 26 April 2024</td>\n",
       "      <td>5.0 out of 5 stars</td>\n",
       "      <td>Good camera, nice performance, excellent displ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   reviewer_name                            review_date              rating  \\\n",
       "0   Pankaj Kumar  Reviewed in India on 24 February 2024  5.0 out of 5 stars   \n",
       "1        vaibhav  Reviewed in India on 13 November 2023  5.0 out of 5 stars   \n",
       "2         ApTreX   Reviewed in India on 6 February 2022  5.0 out of 5 stars   \n",
       "3         Faiyaz    Reviewed in India on 5 January 2024  4.0 out of 5 stars   \n",
       "4  Avnish Shukla     Reviewed in India on 26 April 2024  5.0 out of 5 stars   \n",
       "\n",
       "                                         review_text  \n",
       "0  The iPhone 13 128GB has surpassed my expectati...  \n",
       "1  I snagged the iPhone 13 during the Great India...  \n",
       "2  My honest review after going broke buying this...  \n",
       "3  Design:\\nThe iPhone 13 retains the iconic desi...  \n",
       "4  Good camera, nice performance, excellent displ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_amazon.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewer_name</th>\n",
       "      <th>review_title</th>\n",
       "      <th>rating</th>\n",
       "      <th>review_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sagar Behera</td>\n",
       "      <td>Best in the market!</td>\n",
       "      <td>5</td>\n",
       "      <td>Go fr it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Anshul Duhan</td>\n",
       "      <td>Must buy!</td>\n",
       "      <td>5</td>\n",
       "      <td>Best in class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gundabattina SaradhiMuneendra</td>\n",
       "      <td>Mind-blowing purchase</td>\n",
       "      <td>5</td>\n",
       "      <td>Fabulous üòçLoved itCamera awesome üòòPerformance ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ashutosh  Singh</td>\n",
       "      <td>Must buy!</td>\n",
       "      <td>5</td>\n",
       "      <td>Blue colour is very lightBut performance is ve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Anirudhya  Ghosh</td>\n",
       "      <td>Worth every penny</td>\n",
       "      <td>5</td>\n",
       "      <td>Premium Colour</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   reviewer_name           review_title rating  \\\n",
       "0                   Sagar Behera    Best in the market!      5   \n",
       "1                   Anshul Duhan              Must buy!      5   \n",
       "2  Gundabattina SaradhiMuneendra  Mind-blowing purchase      5   \n",
       "3                Ashutosh  Singh              Must buy!      5   \n",
       "4               Anirudhya  Ghosh      Worth every penny      5   \n",
       "\n",
       "                                         review_text  \n",
       "0                                           Go fr it  \n",
       "1                                      Best in class  \n",
       "2  Fabulous üòçLoved itCamera awesome üòòPerformance ...  \n",
       "3  Blue colour is very lightBut performance is ve...  \n",
       "4                                     Premium Colour  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_flipkart.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test.py\n",
    "import streamlit as st\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "# Web driver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "def scrape_amazon_reviews(url, max_page=5):\n",
    "    reviews = []\n",
    "    for page in range(1, max_page + 1):\n",
    "        print(\"Page:\", page)\n",
    "        page_url = f'{url}{page}?ie=UTF8&reviewerType=all_reviews&pageNumber={page}'\n",
    "        driver.get(page_url)\n",
    "\n",
    "        review_elements = driver.find_elements(By.XPATH, \"//div[@data-hook='review']\")\n",
    "        if review_elements:\n",
    "            for review in review_elements:\n",
    "                review_data_dict = {}\n",
    "                review_data_dict['reviewer_name'] = review.find_element(By.XPATH, \".//span[@class='a-profile-name']\").text\n",
    "                review_data_dict['review_date'] = review.find_element(By.XPATH, \".//span[@data-hook='review-date']\").text\n",
    "                review_data_dict['rating'] = review.find_element(By.XPATH, \".//i[contains(@class, 'review-rating')]/span\").get_attribute('innerText')\n",
    "                review_data_dict['review_text'] = review.find_element(By.XPATH, \".//span[@data-hook='review-body']\").text\n",
    "                reviews.append(review_data_dict)\n",
    "        else:\n",
    "            print(f'NO Reviews Found on Page {page}')\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            next_button = driver.find_element(By.XPATH, \"//li[@class='a-last']//a\")\n",
    "            next_button.click()\n",
    "            WebDriverWait(driver, 10).until(EC.url_changes(page_url))\n",
    "        except NoSuchElementException:\n",
    "            print(\"Next Page not found\")\n",
    "            break\n",
    "\n",
    "    return reviews\n",
    "\n",
    "def scrape_flipkart_reviews(url, max_page=5):\n",
    "    reviews = []\n",
    "    headers = {\n",
    "        'User-Agent': 'Use your own user agent',\n",
    "        'Accept-Language': 'en-us,en;q=0.5'\n",
    "    }\n",
    "\n",
    "    for i in range(1, max_page + 1):\n",
    "        page_url = f'{url}{i}'\n",
    "        page = requests.get(page_url, headers=headers)\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "        names = soup.find_all('p', class_='_2NsDsF AwS1CA')\n",
    "        titles = soup.find_all('p', class_='z9E0IG')\n",
    "        ratings = soup.find_all('div', class_=['XQDdHH Ga3i8K', 'XQDdHH Czs3gR Ga3i8K' , 'XQDdHH Js30Fc Ga3i8K'])\n",
    "        comments = soup.find_all('div', class_='ZmyHeo')\n",
    "\n",
    "        for name, title, rating, comment in zip(names, titles, ratings, comments):\n",
    "            review_data_dict = {}\n",
    "            review_data_dict['reviewer_name'] = name.get_text()\n",
    "            review_data_dict['review_title'] = title.get_text()\n",
    "            review_data_dict['rating'] = rating.get_text() if rating else '0'\n",
    "            review_data_dict['review_text'] = comment.div.div.get_text(strip=True)\n",
    "            reviews.append(review_data_dict)\n",
    "\n",
    "    return reviews\n",
    "\n",
    "# Streamlit code\n",
    "st.title('Web Scraping App')\n",
    "\n",
    "option = st.sidebar.selectbox(\n",
    "    'Which website do you want to scrape?',\n",
    "    ('Amazon', 'Flipkart')\n",
    ")\n",
    "\n",
    "url = st.sidebar.text_input('Enter the URL of the product')\n",
    "\n",
    "if st.sidebar.button('Scrape'):\n",
    "    if option == 'Amazon':\n",
    "        reviews = scrape_amazon_reviews(url)\n",
    "    else:\n",
    "        reviews = scrape_flipkart_reviews(url)\n",
    "\n",
    "    df = pd.DataFrame(reviews)\n",
    "    st.write(df)\n",
    "\n",
    "\n",
    "\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# with sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_senti.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_senti.py\n",
    "import streamlit as st\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import google.generativeai as genai\n",
    "import google.ai.generativelanguage as glm\n",
    "import pandas as pd\n",
    "\n",
    "genai.configure(api_key=\"AIzaSyAVdmSlrL8PA62m--SllyfcOACQ5S2ws5U\")  #AIzaSyD9Tj4yxSUTFYRZaFtPnqCaiWUgMW3m4J4\n",
    "\n",
    "\n",
    "# Web driver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "def scrape_amazon_reviews(url, max_page=5):\n",
    "    reviews = []\n",
    "    for page in range(1, max_page + 1):\n",
    "        print(\"Page:\", page)\n",
    "        page_url = f'{url}{page}?ie=UTF8&reviewerType=all_reviews&pageNumber={page}'\n",
    "        driver.get(page_url)\n",
    "\n",
    "        review_elements = driver.find_elements(By.XPATH, \"//div[@data-hook='review']\")\n",
    "        if review_elements:\n",
    "            for review in review_elements:\n",
    "                review_data_dict = {}\n",
    "                review_data_dict['reviewer_name'] = review.find_element(By.XPATH, \".//span[@class='a-profile-name']\").text\n",
    "                review_data_dict['review_date'] = review.find_element(By.XPATH, \".//span[@data-hook='review-date']\").text\n",
    "                review_data_dict['rating'] = review.find_element(By.XPATH, \".//i[contains(@class, 'review-rating')]/span\").get_attribute('innerText')\n",
    "                review_data_dict['review_text'] = review.find_element(By.XPATH, \".//span[@data-hook='review-body']\").text\n",
    "                reviews.append(review_data_dict)\n",
    "        else:\n",
    "            print(f'NO Reviews Found on Page {page}')\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            next_button = driver.find_element(By.XPATH, \"//li[@class='a-last']//a\")\n",
    "            next_button.click()\n",
    "            WebDriverWait(driver, 10).until(EC.url_changes(page_url))\n",
    "        except NoSuchElementException:\n",
    "            print(\"Next Page not found\")\n",
    "            break\n",
    "\n",
    "    return reviews\n",
    "\n",
    "def scrape_flipkart_reviews(url, max_page=5):\n",
    "    reviews = []\n",
    "    headers = {\n",
    "        'User-Agent': 'Use your own user agent',\n",
    "        'Accept-Language': 'en-us,en;q=0.5'\n",
    "    }\n",
    "\n",
    "    for i in range(1, max_page + 1):\n",
    "        page_url = f'{url}{i}'\n",
    "        page = requests.get(page_url, headers=headers)\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "        names = soup.find_all('p', class_='_2NsDsF AwS1CA')\n",
    "        titles = soup.find_all('p', class_='z9E0IG')\n",
    "        ratings = soup.find_all('div', class_=['XQDdHH Ga3i8K', 'XQDdHH Czs3gR Ga3i8K' , 'XQDdHH Js30Fc Ga3i8K'])\n",
    "        comments = soup.find_all('div', class_='ZmyHeo')\n",
    "\n",
    "        for name, title, rating, comment in zip(names, titles, ratings, comments):\n",
    "            review_data_dict = {}\n",
    "            review_data_dict['reviewer_name'] = name.get_text()\n",
    "            review_data_dict['review_title'] = title.get_text()\n",
    "            review_data_dict['rating'] = rating.get_text() if rating else '0'\n",
    "            review_data_dict['review_text'] = comment.div.div.get_text(strip=True)\n",
    "            reviews.append(review_data_dict)\n",
    "\n",
    "    return reviews\n",
    "\n",
    "# Streamlit code\n",
    "st.title('Web Scraping & Sentimental Analysis App')\n",
    "\n",
    "option = st.sidebar.selectbox(\n",
    "    'Which website do you want to scrape?',\n",
    "    ('Amazon', 'Flipkart')\n",
    ")\n",
    "\n",
    "url = st.sidebar.text_input('Enter the URL of the product')\n",
    "\n",
    "if st.sidebar.button('Scrape'):\n",
    "    if option == 'Amazon':\n",
    "        reviews = scrape_amazon_reviews(url)\n",
    "    else:\n",
    "        reviews = scrape_flipkart_reviews(url)\n",
    "\n",
    "    df = pd.DataFrame(reviews)\n",
    "\n",
    "    # st.write(df)\n",
    "\n",
    "    # st.header(\"Sentiment Analysis of Reviews\")\n",
    "    data = df\n",
    "    sentiment = []\n",
    "    for reviews in df['review_text']:\n",
    "\n",
    "             # model selection\n",
    "        model = genai.GenerativeModel('gemini-pro')\n",
    "\n",
    "        prompt = \"\"\" Give the Sentiment analysis of given review only in two words either ***POSITIVE** üòÄüòÄ  or **NEGATIVE** ‚òπÔ∏è‚òπÔ∏è \"\"\"\n",
    "\n",
    "        response = model.generate_content([prompt, reviews])\n",
    "            #  st.write(reviews)\n",
    "            #  st.write(response.text)\n",
    "        sentiment.append(response.text)\n",
    "            #  st.write(\"=====================================================================================\")\n",
    "    df['Sentiment'] = sentiment \n",
    "\n",
    "    st.write(df)\n",
    "\n",
    "\n",
    "\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
