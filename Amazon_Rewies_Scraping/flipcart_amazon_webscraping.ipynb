{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NoSuchWindowException",
     "evalue": "Message: no such window: target window already closed\nfrom unknown error: web view not found\n  (Session info: chrome=125.0.6422.60)\nStacktrace:\n\tGetHandleVerifier [0x00007FF7340622C2+60002]\n\t(No symbol) [0x00007FF733FDCA59]\n\t(No symbol) [0x00007FF733E97EDA]\n\t(No symbol) [0x00007FF733E6D5B5]\n\t(No symbol) [0x00007FF733F13727]\n\t(No symbol) [0x00007FF733F2B3A1]\n\t(No symbol) [0x00007FF733F0C033]\n\t(No symbol) [0x00007FF733ED9657]\n\t(No symbol) [0x00007FF733EDA251]\n\tGetHandleVerifier [0x00007FF734373E2D+3278285]\n\tGetHandleVerifier [0x00007FF7343C0190+3590448]\n\tGetHandleVerifier [0x00007FF7343B61D0+3549552]\n\tGetHandleVerifier [0x00007FF734111DE6+779654]\n\t(No symbol) [0x00007FF733FE7ACF]\n\t(No symbol) [0x00007FF733FE2EE4]\n\t(No symbol) [0x00007FF733FE3072]\n\t(No symbol) [0x00007FF733FD2C4F]\n\tBaseThreadInitThunk [0x00007FFAAE7B7344+20]\n\tRtlUserThreadStart [0x00007FFAAF4C26B1+33]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNoSuchWindowException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 73\u001b[0m\n\u001b[0;32m     70\u001b[0m amazon_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.amazon.in/Apple-iPhone-13-128GB-Blue/product-reviews/B09G9BL5CP/ref=cm_cr_arp_d_paging_btm_next_\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     71\u001b[0m flipkart_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.flipkart.com/apple-iphone-15-plus-black-256-gb/product-reviews/itm4b0608e773fc5?pid=MOBGTAGPWKT2VSBB&lid=LSTMOBGTAGPWKT2VSBBYV0FGC&marketplace=FLIPKART&page=\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 73\u001b[0m amazon_reviews \u001b[38;5;241m=\u001b[39m scrape_amazon_reviews(amazon_url)\n\u001b[0;32m     74\u001b[0m flipkart_reviews \u001b[38;5;241m=\u001b[39m scrape_flipkart_reviews(flipkart_url)\n\u001b[0;32m     76\u001b[0m driver\u001b[38;5;241m.\u001b[39mclose()\n",
      "Cell \u001b[1;32mIn[3], line 36\u001b[0m, in \u001b[0;36mscrape_amazon_reviews\u001b[1;34m(url, max_page)\u001b[0m\n\u001b[0;32m     34\u001b[0m     next_button \u001b[38;5;241m=\u001b[39m driver\u001b[38;5;241m.\u001b[39mfind_element(By\u001b[38;5;241m.\u001b[39mXPATH, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m//li[@class=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma-last\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m]//a\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     35\u001b[0m     next_button\u001b[38;5;241m.\u001b[39mclick()\n\u001b[1;32m---> 36\u001b[0m     WebDriverWait(driver, \u001b[38;5;241m10\u001b[39m)\u001b[38;5;241m.\u001b[39muntil(EC\u001b[38;5;241m.\u001b[39murl_changes(page_url))\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m NoSuchElementException:\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNext Page not found\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\support\\wait.py:96\u001b[0m, in \u001b[0;36mWebDriverWait.until\u001b[1;34m(self, method, message)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 96\u001b[0m         value \u001b[38;5;241m=\u001b[39m method(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_driver)\n\u001b[0;32m     97\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m value:\n\u001b[0;32m     98\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m value\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\support\\expected_conditions.py:138\u001b[0m, in \u001b[0;36murl_changes.<locals>._predicate\u001b[1;34m(driver)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_predicate\u001b[39m(driver: WebDriver):\n\u001b[1;32m--> 138\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m url \u001b[38;5;241m!=\u001b[39m driver\u001b[38;5;241m.\u001b[39mcurrent_url\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:437\u001b[0m, in \u001b[0;36mWebDriver.current_url\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    428\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m    429\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcurrent_url\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m    430\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Gets the URL of the current page.\u001b[39;00m\n\u001b[0;32m    431\u001b[0m \n\u001b[0;32m    432\u001b[0m \u001b[38;5;124;03m    :Usage:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    435\u001b[0m \u001b[38;5;124;03m            driver.current_url\u001b[39;00m\n\u001b[0;32m    436\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 437\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecute(Command\u001b[38;5;241m.\u001b[39mGET_CURRENT_URL)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:347\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    345\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_executor\u001b[38;5;241m.\u001b[39mexecute(driver_command, params)\n\u001b[0;32m    346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[1;32m--> 347\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_handler\u001b[38;5;241m.\u001b[39mcheck_response(response)\n\u001b[0;32m    348\u001b[0m     response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unwrap_value(response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    349\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py:229\u001b[0m, in \u001b[0;36mErrorHandler.check_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    227\u001b[0m         alert_text \u001b[38;5;241m=\u001b[39m value[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malert\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[38;5;66;03m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[1;32m--> 229\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[1;31mNoSuchWindowException\u001b[0m: Message: no such window: target window already closed\nfrom unknown error: web view not found\n  (Session info: chrome=125.0.6422.60)\nStacktrace:\n\tGetHandleVerifier [0x00007FF7340622C2+60002]\n\t(No symbol) [0x00007FF733FDCA59]\n\t(No symbol) [0x00007FF733E97EDA]\n\t(No symbol) [0x00007FF733E6D5B5]\n\t(No symbol) [0x00007FF733F13727]\n\t(No symbol) [0x00007FF733F2B3A1]\n\t(No symbol) [0x00007FF733F0C033]\n\t(No symbol) [0x00007FF733ED9657]\n\t(No symbol) [0x00007FF733EDA251]\n\tGetHandleVerifier [0x00007FF734373E2D+3278285]\n\tGetHandleVerifier [0x00007FF7343C0190+3590448]\n\tGetHandleVerifier [0x00007FF7343B61D0+3549552]\n\tGetHandleVerifier [0x00007FF734111DE6+779654]\n\t(No symbol) [0x00007FF733FE7ACF]\n\t(No symbol) [0x00007FF733FE2EE4]\n\t(No symbol) [0x00007FF733FE3072]\n\t(No symbol) [0x00007FF733FD2C4F]\n\tBaseThreadInitThunk [0x00007FFAAE7B7344+20]\n\tRtlUserThreadStart [0x00007FFAAF4C26B1+33]\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Web driver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "def scrape_amazon_reviews(url, max_page=3):\n",
    "    reviews = []\n",
    "    for page in range(1, max_page + 1):\n",
    "        # print(\"Page:\", page)\n",
    "        page_url = f'{url}{page}?ie=UTF8&reviewerType=all_reviews&pageNumber={page}'\n",
    "        driver.get(page_url)\n",
    "\n",
    "        review_elements = driver.find_elements(By.XPATH, \"//div[@data-hook='review']\")\n",
    "        if review_elements:\n",
    "            for review in review_elements:\n",
    "                review_data_dict = {}\n",
    "                review_data_dict['reviewer_name'] = review.find_element(By.XPATH, \".//span[@class='a-profile-name']\").text\n",
    "                review_data_dict['review_date'] = review.find_element(By.XPATH, \".//span[@data-hook='review-date']\").text\n",
    "                review_data_dict['rating'] = review.find_element(By.XPATH, \".//i[contains(@class, 'review-rating')]/span\").get_attribute('innerText')\n",
    "                review_data_dict['review_text'] = review.find_element(By.XPATH, \".//span[@data-hook='review-body']\").text\n",
    "                reviews.append(review_data_dict)\n",
    "        else:\n",
    "            print(f'NO Reviews Found on Page {page}')\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            next_button = driver.find_element(By.XPATH, \"//li[@class='a-last']//a\")\n",
    "            next_button.click()\n",
    "            WebDriverWait(driver, 10).until(EC.url_changes(page_url))\n",
    "        except NoSuchElementException:\n",
    "            print(\"Next Page not found\")\n",
    "            break\n",
    "\n",
    "    return reviews\n",
    "\n",
    "def scrape_flipkart_reviews(url, max_page=5):\n",
    "    reviews = []\n",
    "    headers = {\n",
    "        'User-Agent': 'Use your own user agent',\n",
    "        'Accept-Language': 'en-us,en;q=0.5'\n",
    "    }\n",
    "\n",
    "    for i in range(1, max_page + 1):\n",
    "        page_url = f'{url}{i}'\n",
    "        page = requests.get(page_url, headers=headers)\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "        names = soup.find_all('p', class_='_2NsDsF AwS1CA')\n",
    "        titles = soup.find_all('p', class_='z9E0IG')\n",
    "        ratings = soup.find_all('div', class_=['XQDdHH Ga3i8K', 'XQDdHH Czs3gR Ga3i8K' , 'XQDdHH Js30Fc Ga3i8K'])\n",
    "        comments = soup.find_all('div', class_='ZmyHeo')\n",
    "\n",
    "        for name, title, rating, comment in zip(names, titles, ratings, comments):\n",
    "            review_data_dict = {}\n",
    "            review_data_dict['reviewer_name'] = name.get_text()\n",
    "            review_data_dict['review_title'] = title.get_text()\n",
    "            review_data_dict['rating'] = rating.get_text() if rating else '0'\n",
    "            review_data_dict['review_text'] = comment.div.div.get_text(strip=True)\n",
    "            reviews.append(review_data_dict)\n",
    "\n",
    "    return reviews\n",
    "\n",
    "amazon_url = \"https://www.amazon.in/Apple-iPhone-13-128GB-Blue/product-reviews/B09G9BL5CP/ref=cm_cr_arp_d_paging_btm_next_\"\n",
    "flipkart_url = \"https://www.flipkart.com/apple-iphone-15-plus-black-256-gb/product-reviews/itm4b0608e773fc5?pid=MOBGTAGPWKT2VSBB&lid=LSTMOBGTAGPWKT2VSBBYV0FGC&marketplace=FLIPKART&page=\"\n",
    "\n",
    "amazon_reviews = scrape_amazon_reviews(amazon_url)\n",
    "flipkart_reviews = scrape_flipkart_reviews(flipkart_url)\n",
    "\n",
    "driver.close()\n",
    "\n",
    "df_amazon = pd.DataFrame(amazon_reviews)\n",
    "df_flipkart = pd.DataFrame(flipkart_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewer_name</th>\n",
       "      <th>review_date</th>\n",
       "      <th>rating</th>\n",
       "      <th>review_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pankaj Kumar</td>\n",
       "      <td>Reviewed in India on 24 February 2024</td>\n",
       "      <td>5.0 out of 5 stars</td>\n",
       "      <td>The iPhone 13 128GB has surpassed my expectati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vaibhav</td>\n",
       "      <td>Reviewed in India on 13 November 2023</td>\n",
       "      <td>5.0 out of 5 stars</td>\n",
       "      <td>I snagged the iPhone 13 during the Great India...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ApTreX</td>\n",
       "      <td>Reviewed in India on 6 February 2022</td>\n",
       "      <td>5.0 out of 5 stars</td>\n",
       "      <td>My honest review after going broke buying this...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Faiyaz</td>\n",
       "      <td>Reviewed in India on 5 January 2024</td>\n",
       "      <td>4.0 out of 5 stars</td>\n",
       "      <td>Design:\\nThe iPhone 13 retains the iconic desi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Avnish Shukla</td>\n",
       "      <td>Reviewed in India on 26 April 2024</td>\n",
       "      <td>5.0 out of 5 stars</td>\n",
       "      <td>Good camera, nice performance, excellent displ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   reviewer_name                            review_date              rating  \\\n",
       "0   Pankaj Kumar  Reviewed in India on 24 February 2024  5.0 out of 5 stars   \n",
       "1        vaibhav  Reviewed in India on 13 November 2023  5.0 out of 5 stars   \n",
       "2         ApTreX   Reviewed in India on 6 February 2022  5.0 out of 5 stars   \n",
       "3         Faiyaz    Reviewed in India on 5 January 2024  4.0 out of 5 stars   \n",
       "4  Avnish Shukla     Reviewed in India on 26 April 2024  5.0 out of 5 stars   \n",
       "\n",
       "                                         review_text  \n",
       "0  The iPhone 13 128GB has surpassed my expectati...  \n",
       "1  I snagged the iPhone 13 during the Great India...  \n",
       "2  My honest review after going broke buying this...  \n",
       "3  Design:\\nThe iPhone 13 retains the iconic desi...  \n",
       "4  Good camera, nice performance, excellent displ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_amazon.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewer_name</th>\n",
       "      <th>review_title</th>\n",
       "      <th>rating</th>\n",
       "      <th>review_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sagar Behera</td>\n",
       "      <td>Best in the market!</td>\n",
       "      <td>5</td>\n",
       "      <td>Go fr it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Anshul Duhan</td>\n",
       "      <td>Must buy!</td>\n",
       "      <td>5</td>\n",
       "      <td>Best in class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gundabattina SaradhiMuneendra</td>\n",
       "      <td>Mind-blowing purchase</td>\n",
       "      <td>5</td>\n",
       "      <td>Fabulous üòçLoved itCamera awesome üòòPerformance ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ashutosh  Singh</td>\n",
       "      <td>Must buy!</td>\n",
       "      <td>5</td>\n",
       "      <td>Blue colour is very lightBut performance is ve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Anirudhya  Ghosh</td>\n",
       "      <td>Worth every penny</td>\n",
       "      <td>5</td>\n",
       "      <td>Premium Colour</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   reviewer_name           review_title rating  \\\n",
       "0                   Sagar Behera    Best in the market!      5   \n",
       "1                   Anshul Duhan              Must buy!      5   \n",
       "2  Gundabattina SaradhiMuneendra  Mind-blowing purchase      5   \n",
       "3                Ashutosh  Singh              Must buy!      5   \n",
       "4               Anirudhya  Ghosh      Worth every penny      5   \n",
       "\n",
       "                                         review_text  \n",
       "0                                           Go fr it  \n",
       "1                                      Best in class  \n",
       "2  Fabulous üòçLoved itCamera awesome üòòPerformance ...  \n",
       "3  Blue colour is very lightBut performance is ve...  \n",
       "4                                     Premium Colour  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_flipkart.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test.py\n",
    "import streamlit as st\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "# Web driver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "def scrape_amazon_reviews(url, max_page=5):\n",
    "    reviews = []\n",
    "    for page in range(1, max_page + 1):\n",
    "        print(\"Page:\", page)\n",
    "        page_url = f'{url}{page}?ie=UTF8&reviewerType=all_reviews&pageNumber={page}'\n",
    "        driver.get(page_url)\n",
    "\n",
    "        review_elements = driver.find_elements(By.XPATH, \"//div[@data-hook='review']\")\n",
    "        if review_elements:\n",
    "            for review in review_elements:\n",
    "                review_data_dict = {}\n",
    "                review_data_dict['reviewer_name'] = review.find_element(By.XPATH, \".//span[@class='a-profile-name']\").text\n",
    "                review_data_dict['review_date'] = review.find_element(By.XPATH, \".//span[@data-hook='review-date']\").text\n",
    "                review_data_dict['rating'] = review.find_element(By.XPATH, \".//i[contains(@class, 'review-rating')]/span\").get_attribute('innerText')\n",
    "                review_data_dict['review_text'] = review.find_element(By.XPATH, \".//span[@data-hook='review-body']\").text\n",
    "                reviews.append(review_data_dict)\n",
    "        else:\n",
    "            print(f'NO Reviews Found on Page {page}')\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            next_button = driver.find_element(By.XPATH, \"//li[@class='a-last']//a\")\n",
    "            next_button.click()\n",
    "            WebDriverWait(driver, 10).until(EC.url_changes(page_url))\n",
    "        except NoSuchElementException:\n",
    "            print(\"Next Page not found\")\n",
    "            break\n",
    "\n",
    "    return reviews\n",
    "\n",
    "def scrape_flipkart_reviews(url, max_page=5):\n",
    "    reviews = []\n",
    "    headers = {\n",
    "        'User-Agent': 'Use your own user agent',\n",
    "        'Accept-Language': 'en-us,en;q=0.5'\n",
    "    }\n",
    "\n",
    "    for i in range(1, max_page + 1):\n",
    "        page_url = f'{url}{i}'\n",
    "        page = requests.get(page_url, headers=headers)\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "        names = soup.find_all('p', class_='_2NsDsF AwS1CA')\n",
    "        titles = soup.find_all('p', class_='z9E0IG')\n",
    "        ratings = soup.find_all('div', class_=['XQDdHH Ga3i8K', 'XQDdHH Czs3gR Ga3i8K' , 'XQDdHH Js30Fc Ga3i8K'])\n",
    "        comments = soup.find_all('div', class_='ZmyHeo')\n",
    "\n",
    "        for name, title, rating, comment in zip(names, titles, ratings, comments):\n",
    "            review_data_dict = {}\n",
    "            review_data_dict['reviewer_name'] = name.get_text()\n",
    "            review_data_dict['review_title'] = title.get_text()\n",
    "            review_data_dict['rating'] = rating.get_text() if rating else '0'\n",
    "            review_data_dict['review_text'] = comment.div.div.get_text(strip=True)\n",
    "            reviews.append(review_data_dict)\n",
    "\n",
    "    return reviews\n",
    "\n",
    "# Streamlit code\n",
    "st.title('Web Scraping App')\n",
    "\n",
    "option = st.sidebar.selectbox(\n",
    "    'Which website do you want to scrape?',\n",
    "    ('Amazon', 'Flipkart')\n",
    ")\n",
    "\n",
    "url = st.sidebar.text_input('Enter the URL of the product')\n",
    "\n",
    "if st.sidebar.button('Scrape'):\n",
    "    if option == 'Amazon':\n",
    "        reviews = scrape_amazon_reviews(url)\n",
    "    else:\n",
    "        reviews = scrape_flipkart_reviews(url)\n",
    "\n",
    "    df = pd.DataFrame(reviews)\n",
    "    st.write(df)\n",
    "\n",
    "\n",
    "\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# with sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_senti.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_senti.py\n",
    "import streamlit as st\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import google.generativeai as genai\n",
    "import google.ai.generativelanguage as glm\n",
    "import pandas as pd\n",
    "\n",
    "genai.configure(api_key=\"AIzaSyCKESnAOIkl8fIqV-KHOUx5pF-ythxJ1Ng\")  #AIzaSyD9Tj4yxSUTFYRZaFtPnqCaiWUgMW3m4J4\n",
    "\n",
    "\n",
    "# Web driver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "def scrape_amazon_reviews(url, max_page=1):\n",
    "    reviews = []\n",
    "    for page in range(1, max_page + 1):\n",
    "        print(\"Page:\", page)\n",
    "        page_url = f'{url}{page}?ie=UTF8&reviewerType=all_reviews&pageNumber={page}'\n",
    "        driver.get(page_url)\n",
    "\n",
    "        review_elements = driver.find_elements(By.XPATH, \"//div[@data-hook='review']\")\n",
    "        if review_elements:\n",
    "            for review in review_elements:\n",
    "                review_data_dict = {}\n",
    "                review_data_dict['reviewer_name'] = review.find_element(By.XPATH, \".//span[@class='a-profile-name']\").text\n",
    "                review_data_dict['review_date'] = review.find_element(By.XPATH, \".//span[@data-hook='review-date']\").text\n",
    "                review_data_dict['rating'] = review.find_element(By.XPATH, \".//i[contains(@class, 'review-rating')]/span\").get_attribute('innerText')\n",
    "                review_data_dict['review_text'] = review.find_element(By.XPATH, \".//span[@data-hook='review-body']\").text\n",
    "                reviews.append(review_data_dict)\n",
    "        else:\n",
    "            print(f'NO Reviews Found on Page {page}')\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            next_button = driver.find_element(By.XPATH, \"//li[@class='a-last']//a\")\n",
    "            next_button.click()\n",
    "            WebDriverWait(driver, 10).until(EC.url_changes(page_url))\n",
    "        except NoSuchElementException:\n",
    "            print(\"Next Page not found\")\n",
    "            break\n",
    "\n",
    "    return reviews\n",
    "\n",
    "def scrape_flipkart_reviews(url, max_page=5):\n",
    "    reviews = []\n",
    "    headers = {\n",
    "        'User-Agent': 'Use your own user agent',\n",
    "        'Accept-Language': 'en-us,en;q=0.5'\n",
    "    }\n",
    "\n",
    "    for i in range(1, max_page + 1):\n",
    "        page_url = f'{url}{i}'\n",
    "        page = requests.get(page_url, headers=headers)\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "        names = soup.find_all('p', class_='_2NsDsF AwS1CA')\n",
    "        titles = soup.find_all('p', class_='z9E0IG')\n",
    "        ratings = soup.find_all('div', class_=['XQDdHH Ga3i8K', 'XQDdHH Czs3gR Ga3i8K' , 'XQDdHH Js30Fc Ga3i8K'])\n",
    "        comments = soup.find_all('div', class_='ZmyHeo')\n",
    "\n",
    "        for name, title, rating, comment in zip(names, titles, ratings, comments):\n",
    "            review_data_dict = {}\n",
    "            review_data_dict['reviewer_name'] = name.get_text()\n",
    "            review_data_dict['review_title'] = title.get_text()\n",
    "            review_data_dict['rating'] = rating.get_text() if rating else '0'\n",
    "            review_data_dict['review_text'] = comment.div.div.get_text(strip=True)\n",
    "            reviews.append(review_data_dict)\n",
    "\n",
    "    return reviews\n",
    "\n",
    "# Streamlit code\n",
    "st.title('Web Scraping & Sentimental Analysis App')\n",
    "\n",
    "option = st.sidebar.selectbox(\n",
    "    'Which website do you want to scrape?',\n",
    "    ('Amazon', 'Flipkart')\n",
    ")\n",
    "\n",
    "url = st.sidebar.text_input('Enter the URL of the product')\n",
    "\n",
    "if st.sidebar.button('Scrape'):\n",
    "    if option == 'Amazon':\n",
    "        reviews = scrape_amazon_reviews(url)\n",
    "    else:\n",
    "        reviews = scrape_flipkart_reviews(url)\n",
    "\n",
    "    df = pd.DataFrame(reviews)\n",
    "\n",
    "    # st.write(df)\n",
    "\n",
    "    # st.header(\"Sentiment Analysis of Reviews\")\n",
    "    data = df\n",
    "    sentiment = []\n",
    "    for reviews in df['review_text']:\n",
    "\n",
    "             # model selection\n",
    "        model = genai.GenerativeModel('gemini-1.5-flash-latest')\n",
    "\n",
    "        prompt = \"\"\" Give the Sentiment analysis of given review only in two words either ***POSITIVE** üòÄüòÄ  or **NEGATIVE** ‚òπÔ∏è‚òπÔ∏è \"\"\"\n",
    "\n",
    "        response = model.generate_content([prompt, reviews])\n",
    "            #  st.write(reviews)\n",
    "            #  st.write(response.text)\n",
    "        sentiment.append(response.text)\n",
    "            #  st.write(\"=====================================================================================\")\n",
    "    df['Sentiment'] = sentiment \n",
    "\n",
    "    st.write(df)\n",
    "\n",
    "\n",
    "\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
