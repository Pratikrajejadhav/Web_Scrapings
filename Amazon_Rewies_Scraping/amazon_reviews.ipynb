{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from selenium import webdriver\n",
    "# from selenium.webdriver.common.by import By\n",
    "# from selenium.webdriver.support.ui import WebDriverWait\n",
    "# from selenium.webdriver.support import expected_conditions as EC\n",
    "# from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "# # Web driver\n",
    "# driver = webdriver.Chrome()\n",
    "\n",
    "# def scrape_reviews(url, max_page=5):\n",
    "#     reviews = []\n",
    "\n",
    "#     for page in range(1, max_page + 1):\n",
    "#         print(\"Page:\", page)\n",
    "#         page_url = f'{url}{page}?ie=UTF8&reviewerType=all_reviews&pageNumber={page}'\n",
    "#         driver.get(page_url)\n",
    "\n",
    "#         review_elements = driver.find_elements(By.XPATH, \"//div[@data-hook='review']\")\n",
    "#         if review_elements:\n",
    "#             for review in review_elements:\n",
    "#                 review_data_dict = {}\n",
    "#                 review_data_dict['reviewer_name'] = review.find_element(By.XPATH, \".//span[@class='a-profile-name']\").text\n",
    "#                 review_data_dict['review_date'] = review.find_element(By.XPATH, \".//span[@data-hook='review-date']\").text\n",
    "              \n",
    "#                 review_data_dict['rating'] = review.find_element(By.XPATH, \".//i[contains(@class, 'review-rating')]/span\").get_attribute('innerText')\n",
    "                \n",
    "#                 # review_data_dict['rating'] = \"Rating not found\"\n",
    "#                 review_data_dict['review_text'] = review.find_element(By.XPATH, \".//span[@data-hook='review-body']\").text\n",
    "#                 reviews.append(review_data_dict)\n",
    "#         else:\n",
    "#             print(f'NO Reviews Found on Page {page}')\n",
    "#             break\n",
    "\n",
    "#         # Clicking on the next page button\n",
    "#         try:\n",
    "#             next_button = driver.find_element(By.XPATH, \"//li[@class='a-last']//a\")\n",
    "#             next_button.click()\n",
    "#             WebDriverWait(driver, 10).until(EC.url_changes(page_url))\n",
    "#         except NoSuchElementException:\n",
    "#             print(\"Next Page not found\")\n",
    "#             break\n",
    "\n",
    "#     return reviews\n",
    "\n",
    "# web_page_url = \"https://www.amazon.in/Apple-iPhone-13-128GB-Blue/product-reviews/B09G9BL5CP/ref=cm_cr_arp_d_paging_btm_next_\"\n",
    "\n",
    "\n",
    "# import re\n",
    "\n",
    "# # full_link = \"https://www.amazon.in/Apple-iPhone-13-128GB-Blue/product-reviews/B09G9BL5CP/ref=cm_cr_arp_d_paging_btm_next_1?ie=UTF8&reviewerType=all_reviews&pageNumber=1\"\n",
    "\n",
    "# # Define a regular expression pattern to extract the desired part of the link\n",
    "# pattern = r'(https://www.amazon.in/[^/]+/product-reviews/[^/]+/ref=cm_cr_arp_d_paging_btm_next_)'\n",
    "\n",
    "# # Use re.search to find the pattern in the link\n",
    "# match = re.search(pattern, full_link)\n",
    "\n",
    "# if match:\n",
    "#     extracted_part = match.group(1)\n",
    "#     print(extracted_part)\n",
    "# else:\n",
    "#     print(\"Pattern not found in the link.\")\n",
    "\n",
    "# amazon_reviews = scrape_reviews(web_page_url)\n",
    "\n",
    "# print(amazon_reviews)\n",
    "\n",
    "# driver.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# amazon_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# df = pd.DataFrame(amazon_reviews)\n",
    "\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('amazon_reviews.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for review in df['review_text']:\n",
    "#     print(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting sentiment_analysis.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile sentiment_analysis.py\n",
    "# Importing necessary modules\n",
    "import streamlit as st \n",
    "import google.generativeai as genai\n",
    "import google.ai.generativelanguage as glm\n",
    "import pandas as pd\n",
    "# scraping start \n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import re\n",
    "\n",
    "# Web driver\n",
    "# chrome_options = Options()\n",
    "# chrome_options.add_argument(\"--headless\")\n",
    "\n",
    "\n",
    "def scrape_reviews(url, max_page=5):\n",
    "    driver = webdriver.Chrome()\n",
    "    reviews = []\n",
    "\n",
    "    for page in range(1, max_page + 1):\n",
    "        print(\"Page:\", page)\n",
    "        page_url = f'{url}{page}?ie=UTF8&reviewerType=all_reviews&pageNumber={page}'\n",
    "        driver.get(page_url)\n",
    "\n",
    "        review_elements = driver.find_elements(By.XPATH, \"//div[@data-hook='review']\")\n",
    "        if review_elements:\n",
    "            for review in review_elements:\n",
    "                review_data_dict = {}\n",
    "                review_data_dict['reviewer_name'] = review.find_element(By.XPATH, \".//span[@class='a-profile-name']\").text\n",
    "                review_data_dict['review_date'] = review.find_element(By.XPATH, \".//span[@data-hook='review-date']\").text\n",
    "              \n",
    "                review_data_dict['rating'] = review.find_element(By.XPATH, \".//i[contains(@class, 'review-rating')]/span\").get_attribute('innerText')\n",
    "                \n",
    "                # review_data_dict['rating'] = \"Rating not found\"\n",
    "                review_data_dict['review_text'] = review.find_element(By.XPATH, \".//span[@data-hook='review-body']\").text\n",
    "                reviews.append(review_data_dict)\n",
    "        else:\n",
    "            print(f'NO Reviews Found on Page {page}')\n",
    "            break\n",
    "\n",
    "        # Clicking on the next page button\n",
    "        try:\n",
    "            next_button = driver.find_element(By.XPATH, \"//li[@class='a-last']//a\")\n",
    "            next_button.click()\n",
    "            WebDriverWait(driver, 10).until(EC.url_changes(page_url))\n",
    "        except NoSuchElementException:\n",
    "            print(\"Next Page not found\")\n",
    "            break\n",
    "        \n",
    "    driver.close()\n",
    "\n",
    "    return reviews\n",
    "\n",
    "# web_page_url = \"https://www.amazon.in/Apple-iPhone-13-128GB-Blue/product-reviews/B09G9BL5CP/ref=cm_cr_arp_d_paging_btm_next_\"\n",
    "\n",
    "\n",
    "\n",
    "# scraping ends here\n",
    "\n",
    "\n",
    "genai.configure(api_key=\"<your api key>\")\n",
    "\n",
    "if __name__=='__main__':\n",
    "\n",
    "    st.header(\"Sentiment Analysis of Reviews\")\n",
    "    # data = st.sidebar.file_uploader(\"Upload Data File Here\", type=['csv'])\n",
    "\n",
    "    full_link = st.sidebar.text_input(\"Enter the link Here\")\n",
    "    btn =  st.sidebar.button(\"start\")\n",
    "\n",
    "\n",
    "    if btn:\n",
    "\n",
    "        # full_link = \"https://www.amazon.in/Apple-iPhone-13-128GB-Blue/product-reviews/B09G9BL5CP/ref=cm_cr_arp_d_paging_btm_next_1?ie=UTF8&reviewerType=all_reviews&pageNumber=1\"\n",
    "\n",
    "        # Define a regular expression pattern to extract the desired part of the link\n",
    "        pattern = r'(https://www.amazon.in/[^/]+/product-reviews/[^/]+/ref=cm_cr_arp_d_paging_btm_next_)'\n",
    "\n",
    "        # Use re.search to find the pattern in the link\n",
    "        match = re.search(pattern, full_link)\n",
    "\n",
    "        if match:\n",
    "            extracted_part = match.group(1)\n",
    "            print(extracted_part)\n",
    "            amazon_reviews = scrape_reviews(extracted_part)\n",
    "        else:\n",
    "            print(\"Pattern not found in the link.\")\n",
    "\n",
    "        # amazon_reviews = scrape_reviews(web_page_url)\n",
    "\n",
    "        # print(amazon_reviews)\n",
    "\n",
    "\n",
    "\n",
    "        data_file = pd.DataFrame(amazon_reviews)\n",
    "        data_file['date'] = data_file['review_date'].str.extract(r'on (\\d+ \\w+ \\d{4})')\n",
    "        data_file['date'] = pd.to_datetime(data_file['date'], format='%d %B %Y')\n",
    "        if data_file is not None:\n",
    "            # data_file  = pd.read_csv(data)\n",
    "            # data_file = pd.DataFrame(amazon_reviews)\n",
    "            Sentiment = []\n",
    "            for reviews in data_file['review_text']:\n",
    "             \n",
    "                # model selection\n",
    "                model = genai.GenerativeModel('gemini-pro')\n",
    "\n",
    "                prompt = \"\"\" Give the Sentiment analysis of given review only in three words either **POSITIVE**üëçüèª or **NEGATIVE** üëéüèª \n",
    "                consider one more condition if the review is to larger consider it **spam** \"\"\"\n",
    "\n",
    "                response = model.generate_content([prompt, reviews])\n",
    "                #  st.write(reviews)\n",
    "                #  st.write(response.text)\n",
    "                Sentiment.append(response.text)\n",
    "                #  st.write(\"===============================================================================\")\n",
    "\n",
    "            data_file['Sentiment'] = Sentiment\n",
    "            data_file.drop(columns=['review_date'], inplace=True)\n",
    "            data_file = data_file.sort_values(by='date', ascending=False).reset_index(drop=True)\n",
    "            st.write(data_file)\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting timepass.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile timepass.py\n",
    "import streamlit as st\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Function to scrape reviews from Amazon\n",
    "def scrape_amazon_reviews(url, max_page=5):\n",
    "    driver = webdriver.Chrome()\n",
    "    reviews = []\n",
    "\n",
    "    for page in range(1, max_page + 1):\n",
    "        print(\"Page:\", page)\n",
    "        page_url = f'{url}{page}?ie=UTF8&reviewerType=all_reviews&pageNumber={page}'\n",
    "        driver.get(page_url)\n",
    "\n",
    "        review_elements = driver.find_elements(By.XPATH, \"//div[@data-hook='review']\")\n",
    "        if review_elements:\n",
    "            for review in review_elements:\n",
    "                review_data_dict = {}\n",
    "                review_data_dict['reviewer_name'] = review.find_element(By.XPATH, \".//span[@class='a-profile-name']\").text\n",
    "                review_data_dict['review_date'] = review.find_element(By.XPATH, \".//span[@data-hook='review-date']\").text\n",
    "                review_data_dict['rating'] = review.find_element(By.XPATH, \".//i[contains(@class, 'review-rating')]/span\").get_attribute('innerText')\n",
    "                review_data_dict['review_text'] = review.find_element(By.XPATH, \".//span[@data-hook='review-body']\").text\n",
    "                reviews.append(review_data_dict)\n",
    "        else:\n",
    "            print(f'NO Reviews Found on Page {page}')\n",
    "            break\n",
    "\n",
    "        # Clicking on the next page button\n",
    "        try:\n",
    "            next_button = driver.find_element(By.XPATH, \"//li[@class='a-last']//a\")\n",
    "            next_button.click()\n",
    "            WebDriverWait(driver, 10).until(EC.url_changes(page_url))\n",
    "        except NoSuchElementException:\n",
    "            print(\"Next Page not found\")\n",
    "            break\n",
    "        \n",
    "    driver.close()\n",
    "\n",
    "    return reviews\n",
    "\n",
    "# Function to scrape reviews from Flipkart\n",
    "\n",
    "# def scrape_flipkart_reviews(url, max_page=5):\n",
    "#     driver = webdriver.Chrome()\n",
    "#     reviews = []\n",
    "\n",
    "    # for page in range(1, max_page + 1):\n",
    "    #     print(\"Page:\", page)\n",
    "    #     page_url = f'{url}{page}'\n",
    "    #     driver.get(page_url)\n",
    "\n",
    "    #     review_elements = driver.find_elements(By.XPATH, \"//div[@class='col EPCmJX Ma1fCG']\")\n",
    "    #     if review_elements:\n",
    "    #         for review in review_elements:\n",
    "    #             review_data_dict = {}\n",
    "    #             review_data_dict['reviewer_name'] = review.find_element(By.XPATH, \".//p[@class='_2NsDsF AwS1CA']\").text\n",
    "    #             # review_data_dict['review_date'] = review.find_element(By.XPATH, \".//div[@class='row']//p[@class='_35HzRV']\").text\n",
    "    #             review_data_dict['rating'] = review.find_element(By.XPATH, \".//div[@class='XQDdHH Ga3i8K']\").text\n",
    "    #             review_data_dict['review_text'] = review.find_element(By.XPATH, \".//div[@class='ZmyHeo']\").text\n",
    "    #             reviews.append(review_data_dict)\n",
    "    #     else:\n",
    "    #         print(f'NO Reviews Found on Page {page}')\n",
    "    #         break\n",
    "\n",
    "    # driver.close()\n",
    "\n",
    "    # return reviews\n",
    "\n",
    "# Function to scrape reviews from Flipkart\n",
    "def scrape_flipkart_reviews(url, max_page=5):\n",
    "    driver = webdriver.Chrome()\n",
    "    reviews = []\n",
    "\n",
    "    for page in range(1, max_page + 1):\n",
    "        print(\"Page:\", page)\n",
    "        page_url = f'{url}{page}'\n",
    "        driver.get(page_url)\n",
    "\n",
    "        review_elements = driver.find_elements(By.XPATH, \"//div[@class='col EPCmJX Ma1fCG']\")\n",
    "        if review_elements:\n",
    "            for review in review_elements:\n",
    "                review_data_dict = {}\n",
    "                try:\n",
    "                    review_data_dict['reviewer_name'] = review.find_element(By.XPATH, \".//p[@class='_2NsDsF AwS1CA']\").text\n",
    "                    review_data_dict['rating'] = review.find_element(By.XPATH, \".//div[@class='XQDdHH Ga3i8K']\").text\n",
    "                    review_data_dict['review_text'] = review.find_element(By.XPATH, \".//div[@class='ZmyHeo']\").text\n",
    "                    reviews.append(review_data_dict)\n",
    "                except NoSuchElementException:\n",
    "                    print(\"Some elements not found on page\", page)\n",
    "        else:\n",
    "            print(f'NO Reviews Found on Page {page}')\n",
    "            break\n",
    "\n",
    "    driver.close()\n",
    "\n",
    "    return reviews\n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "\n",
    "    st.header(\"Sentiment Analysis of Reviews\")\n",
    "\n",
    "    full_link = st.sidebar.text_input(\"Enter the link Here\")\n",
    "    platform = st.sidebar.selectbox(\"Select Platform\", [\"Amazon\", \"Flipkart\"])\n",
    "    max_page = st.sidebar.number_input(\"Enter Max Pages\", min_value=1, step=1)\n",
    "    btn =  st.sidebar.button(\"Start\")\n",
    "\n",
    "    if btn:\n",
    "        pattern = r'(https://www.amazon.in/[^/]+/product-reviews/[^/]+/ref=cm_cr_arp_d_paging_btm_next_)'\n",
    "        match = re.search(pattern, full_link)\n",
    "\n",
    "        if match:\n",
    "            extracted_part = match.group(1)\n",
    "            print(extracted_part)\n",
    "            if platform == \"Amazon\":\n",
    "                reviews = scrape_amazon_reviews(extracted_part, max_page)\n",
    "            elif platform == \"Flipkart\":\n",
    "                reviews = scrape_flipkart_reviews(full_link, max_page)\n",
    "        else:\n",
    "            print(\"Pattern not found in the link.\")\n",
    "        \n",
    "        \n",
    "        data_file = pd.DataFrame(reviews)\n",
    "        # Perform sentiment analysis and further processing...\n",
    "        st.write(data_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
